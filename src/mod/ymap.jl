
using TensorCast, Einsum

using Statistics

"""
    y(Œ†) = yexp(Œ†, [1,2])

Map the prior `Œ†` to data space `y` of sum-of-exponentials model, with equal
amplitudes, for given list of times. Default `times = [1,‚ÑØ]` as in the paper.

* If the prior has `Œ∏_Œº ‚àà [0..1]`, like `Œ† = wrand(2,20)`,
  then decay constants are `k_Œº = -log(Œ∏_Œº) ‚àà [0,‚àû]`.
* If the prior has unbounded `Œ∏_Œº`, like `Œ† = wrandn(2,20)`,
  then decay constants are `k_Œº = exp(Œ∏_Œº)`.
"""
function yexp(theta::Weighted, times::AbsVec = [1,‚ÑØ])
    if theta.opt.clamp
        theta.opt.lo == 0 && theta.opt.hi == 1 || error("yexp doesn't understand these limits")
        array = yexp_compact(theta.array, times)
    else
        array = yexp_noncompact(theta.array, times)
    end
    Weighted(array, theta.weights, clamp(aname(theta.opt, "y"))) # result y always in [0,1]
end

yexp_compact(theta::AbsMat, times::AbsVec) =
    @reduce yexp[t,a] := mean(i) theta[i,a] ^ times[t]

yexp_noncompact(theta::AbsMat, times::AbsVec) =
    @reduce yexp[t,a] := mean(i) exp(-exp(theta[i,a]) * times[t])

"""
    yexp(Œ∏, [1,2])
    Œ∏ |> yexp([1,2])

Map one `Œ∏::Vector` to data space. By default expects unbounded Œ∏;
for `Œ∏ ‚àà [0,1]` use `compact=true`.
"""
function yexp(theta::AbsVec, times::AbsVec; wrap=false, compact=false)
    wrap==false || error("option wrap=true removed as it was type-unstable!")
    v = compact ?
        [ mean(theta .^ t) for t in times ] :
        [ mean(@. exp( -exp(theta) * t) ) for t in times ]

    # Returns a vector, unless `wrap=true` makes a one-col WeightedMatrix.
    # but that wasn't type-stable!
    # !wrap ?
    #     v :
    #     Weighted(reshape(v,:,1), [1.0], WeightOpt(clamp=true, aname="y"))
end
yexp(times::AbsVec; kw...) = Œ∏ -> yexp(Œ∏, times; kw...)

function yexp!(out::AbsVec, theta::AbsVec{T}, times::AbsVec) where {T}
    id = 1/T(length(theta))
    @fastmath @einsimd out[t] = exp(-exp(theta[Œº]) * times[t]) * id
end

"""
    Œ†_R = loglog(Œ†_01)

This converts a prior with `Œ∏_Œº ‚àà [0..1]` to one with the parameterisation `Œ∏_Œº ‚àà ‚Ñù`,
cut off at `|Œ∏_Œº|<bound`, keyword `bound=10` by default.
Both should give the same result under `yexp(Œ†, times)`.
"""
function loglog(prior::Weighted; bound=10)
    prior.opt.clamp && prior.opt.lo == 0 && prior.opt.hi == 1 || error("loglog expected a prior in 0...1 box")
    theta = @. clamp(log(-log(prior.array)), -bound, bound)
    Weighted(theta, prior.weights, unclamp(prior.opt))
end

#===== Jeffreys prior =====#

"""
    Œ∏ |> jlog_exp(times)
    Œ∏ |> jlog_exp(T, times)

Log Jeffreys density `log(sqrt(det(g_ŒºŒΩ)))` for sum of exponentials model
described by `yexp(Œ†, times)`,
with unbounded `Œ∏ = log.(k) ‚ààùêë·µà` such as a column of `Œ† = wrandn(d,k)`.

`jlog(big, 1:3)` uses `BigFloat` for the calculation.

Curried form is only for convenience, as no work can be re-used.

See also `jlog_vdm`.
"""
jlog_exp(times::AbsVec) = jlog_exp(Float64, times)

jlog_exp(T::F, times::AbsVec) where {F} = theta -> begin #  where {F} forces specialisation
    jac = jacobi_exp(convertarray(T,theta), convertarray(T,times))
    fish = jac' * jac
    if T isa typeof(big) || (T isa DataType && T <: BigFloat) # using logabsdet bigfloats give a crash?
        detg = LinearAlgebra.det(fish)
        detg>0 ? Float64(log(detg)/2) : -Inf
    else
        logdetg, s = LinearAlgebra.logabsdet(fish)
        ifelse(s>0, Float64(logdetg)/2, -Inf)
    end
end

# convertarray(T, x::AbstractArray) = map(T, x) # map preserves ranges, gives problems
convertarray(T, x::AbstractArray) = T.(x) # broadcast does not
convertarray(::Type{T}, x::AbstractArray{T}) where {T} = x

"""
    jacobi_exp(Œ∏, times)

This is the matrix `dy_t / dŒ∏_Œº`, for unbounded Œ∏ parameter vector of `yexp`.
"""
function jacobi_exp(theta::AbsVec{T}, times::AbsVec) where {T}
    exptheta = exp.(theta) # d + d^2 * m calls to exp(), vs. 2 * d^2 * m
    id = 1/T(length(theta))
    @einsum jacobi[t,Œº] := -times[t] * exp(theta[Œº] - exptheta[Œº] * times[t]) * id
    # @vielsum is a bad idea since KissMCMC has its own threaded loop.
end
function jacobi_exp!(jacobi::AbsMat, theta::AbsVec{T}, times::AbsVec; cache=similar(theta)) where {T}
    exptheta = cache .= exp.(theta)
    id = 1/T(length(theta))
    @fastmath @einsimd jacobi[t,Œº] = -times[t] * exp(theta[Œº] - exptheta[Œº] * times[t]) * id
end

"""
    Œ∏ |> jlogpost_exp(x, œÉ, times)
    Œ∏ |> jlogpost_exp(T, x, œÉ, times)

Use this to sample from posterior `p_J(Œ∏|x)` arising from Jeffreys prior `p_J(Œ∏)`,
for the model `yexp(Œ†, times)` + gaussian noise `œÉ`.
Note that `x` is in data space, `length(x)==length(times)==size(yexp(Œ†, times),1)`.

Thus `emcee(jlogpost_exp(x,œÉ,times), Œ†0, N)` would be like `gpost(x, yexp(times), Œ†J, œÉ)`
if you had sufficient sampling of `Œ†J = emcee(jlog_exp(times), Œ†0, N)` near to `x`.

Version with `T` calls `jlog_exp` with this type.

Keyword `alpha=1` (default) gives the posterior density. Smaller values change
the power, down to `alpha=0` which gives the prior instead.

See `flogpost_exp` for flat-prior version.
"""
jlogpost_exp(x::AbsVec, œÉ::Real, times::AbsVec; kw...) = jlogpost_exp(Float64, x, œÉ, times; kw...)

function jlogpost_exp(T::F, x::AbsVec, œÉ::Real, times::AbsVec; alpha::Real=true) where {F}
    @assert length(x)==length(times) "data point x must be in same number of dimensions as y(Œ∏)"
    T == Float64 && @warn "jlogpost_exp gave me some awful answers, because of accuracy... try with Double64 or BigFloat to be sure!" maxlog=3
    Œ∏ -> jlog_exp(T, times)(Œ∏) - alpha * (0.5/œÉ^2) * sum(abs2, x .- yexp(Œ∏, times))
end

"""
    Œ∏ |> jlog_vdm(times)

Log Jeffreys density for sum of exponentials model, but unlike `jlog_exp`
this needs as many times as parameters `m==d`,
evenly spaced `times::AbstractRange` with `first(times)==1`.

Works out `abs(det(J))` instead of `sqrt(det(g))`, and `J` is close enough to Vandermonde.

Gives good results with Float64 where `jlog_exp` needs to be using BigFloat,
and is thus much faster. Extending to `m>d` would involve some
messing with Schur polynomials.
"""
jlog_vdm(times) = jlog_vdm(Float64, times)
jlog_vdm(T::F, times::AbstractRange) where {F} = Œ∏ -> begin # where {F} forces specialisation
    first(times) == 1 || error("times must start at 1")
    d, m = length(Œ∏), length(times)
    m == d || error("number of parameters must match number of times")
    tstep = T(step(times))
    Œ∏pow = @. exp(-tstep * exp(T(Œ∏))) # does 2d calls to exp, and d later, could combine.
    # negs = 0
    out = 0.0
    @inbounds for i in 1:d
        for j in i+1:m
            delta = Œ∏pow[j] - Œ∏pow[i]
            # negs += delta<0 # det(J) gets squared, so negative is fine
            out += log(abs(delta)) |> Float64 # Vandermonde term
        end
    end
    for j in 1:m
        out += log(T(times[j])/d) |> Float64 # diagonal factor
    end
    for i in 1:d
        out += Œ∏[i] - exp(T(Œ∏[i])) |> Float64 # change of variables
    end
    out
end

"""
    Œ∏ |> jlogpost_vdm(T, x, œÉ, times)

Just like `jlogpost_exp`, but using `jlog_vdm` obviously.
"""
jlogpost_vdm(x::AbsVec, œÉ::Real, times::AbsVec; kw...) = jlogpost_vdm(Float64, x, œÉ, times; kw...)
function jlogpost_vdm(T::F, x::AbsVec, œÉ::Real, times::AbsVec; alpha::Real=true) where {F}
    @assert length(x)==length(times) "data point x must be in same number of dimensions as y(Œ∏)"
    Œ∏ -> jlog_vdm(T, times)(Œ∏) - alpha * (0.5/œÉ^2) * sum(abs2, x .- yexp(Œ∏, times))
end

#===== Flat & log-normal priors =====#

"""
    Œ∏ |> flogpost_exp(x, œÉ, times)

Use this to sample from posterior `p_F(Œ∏|x)` arising from a flat prior `p(Œ∏) ‚àù 1`,
for the model `yexp(Œ†, times)` + gaussian noise `œÉ`. Compare `jlogpost_exp` for Jeffreys prior.

Flat meaning flat on the noncompact `Œ∏=log(k)` parameters by default.
`flogpost_exp(x, œÉ, times, :k)` is flat in decay rate instead,
and with `:T` flat in lifetime.
"""
function flogpost_exp(x::AbsVec, œÉ::Real, times::AbsVec, var::Symbol=:Œ∏)
    @assert length(x)==length(times) "data point x must be in same number of dimensions as y(Œ∏)"
    mi2œÉ = -0.5/œÉ^2
    if var === :Œ∏
        Œ∏ ->  mi2œÉ * sum(abs2, x .- yexp(Œ∏, times))
    elseif var === :k
        Œ∏ -> +sum(Œ∏) + mi2œÉ * sum(abs2, x .- yexp(Œ∏, times))
    elseif var === :T
        Œ∏ -> -sum(Œ∏) + mi2œÉ * sum(abs2, x .- yexp(Œ∏, times))
    else
        error("expected symbol in [:Œ∏, :k, :T]")
    end
end

"""
    Œ∏ |> flog_exp(:k)

Log density for sampling from a flat prior, for model `yexp(Œ†, times)`.

Works in terms of the noncompact `Œ∏=log(k)`, which should be restricted by e.g. using `clamp(wrandn(5,10),-5,5)` as the initial points for `emcee`.

With symbol `:k` it is flat in the decay rate, symbol `:T` flat in lifetime instead, default is `:Œ∏`.

Compare `jlog_exp(times)` for Jeffreys prior, and `flogpost_exp(x, œÉ, times)` for posterior.
"""
function flog_exp(var::Symbol=:Œ∏)
    if var === :Œ∏
        Œ∏ -> 1.0
    elseif var === :k
        Œ∏ -> +sum(Œ∏)
    elseif var === :T
        Œ∏ -> -sum(Œ∏)
    else
        error("expected symbol in [:Œ∏, :k, :T]")
    end
end

"""
    Œ∏ |> nlog(Œº=0, œÉ=1)

Log density for sampling from normal distribution, `Œ∏ ~ ùí©(Œº, œÉ¬≤)`.
Used with `yexp(Œ†, times)`, this amounts to a log-normal distribution
for each decay rate `k = exp(Œ∏)`, using `Œº=0.69` moves the centre up to `k=2`.
NB this `œÉ` is the width of the prior, unrelated to the model's noise!

Compare `jlog_exp(times)` for Jeffreys prior, and `flog_exp(:k)` for flat in `k`.
"""
nlog(Œº=false, œÉ::Number=true) = Œ∏ -> sum(abs2.(Œ∏ .- Œº) .* (-1/(2œÉ^2)))

# """
#     Œ∏ |> nlogpost(x, œÉ_mod, Œº=0, œÉ_prior=1)

# Use this to sample from posterior `p_N(Œ∏|x)` arising from
# a normal prior `p_N(Œ∏)` a.k.a. `nlog(Œº, œÉ_prior)`.
# NB `œÉ_mod` is the noise of the model, unrelated to the width of the prior.
# """
# nlogpost(x::AbsVec, œÉ_model, Œº_prior=false, œÉ_prior::Number=true) =
#     Œ∏ -> sum(abs2.(Œ∏ .- x).*(-1/(2œÉ_model^2)) .+ abs2.(Œ∏ .- Œº_prior).*(-1/(2œÉ_prior^2)))

"""
    Œ∏ |> nlogpost_exp(x, œÉ_mod, times, Œº=0, œÉ_prior=1)

Use this to sample from posterior `p_N(Œ∏|x)` arising from
a normal prior `p_N(Œ∏)` a.k.a. `nlog(Œº, œÉ_prior)`.
NB `œÉ_mod` is the noise of the model, unrelated to the width of the prior.

Keyword `alpha=1` (default) gives the posterior density. Smaller values change
the power, down to `alpha=0` which gives the prior instead.
"""
nlogpost_exp(x::AbsVec, œÉ_model, times::AbsVec, Œº_prior=false, œÉ_prior::Number=true; alpha=true) =
    Œ∏ -> begin
        y = yexp(Œ∏, times)
        alpha * sum(abs2.(y .- x).*(-1/(2œÉ_model^2))) + sum(abs2.(Œ∏ .- Œº_prior) .* (-1/(2œÉ_prior^2)))
    end


#===== Faster =====#

using Einsum

using ArrayAllez

using Tracker: Tracker, TrackedMatrix, track, @grad

yexp_compact(theta::TrackedMatrix, times::AbsVec) = track(yexp_compact, theta, times)

@grad function yexp_compact(theta::TrackedMatrix{T}, times::AbsVec) where T # combined fwd+back
    d,k = size(theta)
    nt = length(times)
    id = 1/d
    Œ∏data = theta.data

    powersave = Array_{T}(:powersave, nt,d,k)
    @vielsum powersave[t,i,a] = Œ∏data[i,a] ^ (times[t]-1) # do the expensive bit once

    @einsum yexp[t,a] := id * powersave[t,i,a] * Œ∏data[i,a] # Œ£_i

    function back(Œî) # I think this is not hitting closure bug
        Œîdata = Tracker.data(Œî)
        @einsum ‚àá[i,a] := id * Œîdata[t,a] * times[t] * powersave[t,i,a] # Œ£_t
        (‚àá, nothing)
    end

    yexp, back
end

