
#========== MCMC ==========#

using KissMCMC: KissMCMC

using Printf

"""
    metropolis(logpdf, Œ∏0, n) -> matrix
    metropolis(logpdf, Œ†0, n) -> Weighted

Takes `n` samples according to the given `logpdf(::Vector)`.
If given a `Œ†0::Weighted` instead of an initial vector,
it will read bounds from this, and return another such.

Keywords `step=1.0` size of jump, `burn=n/3` number of burn-in steps,
`nthin=1` keeps every step.

```
metropolis(jlog_exp(1:2), wrandn(2,1), 1_000)
plot(yexp(ans, 1:2))
```
"""
function metropolis(logf::Function, theta0::Vector, n::Int;
        step=1.0, burn=div(n,3), verbose=true, kw...)
    res, acc = KissMCMC.metropolis(logf, Œ∏->Œ∏ .+ step .* randn.(), theta0;
        niter=n+burn, nburnin=burn, kw...)
    @info string("metropolis accepted ", @sprintf("%.2f",100*acc), " percent")
    reduce(hcat, res)
end

function metropolis(logf::Function, x::Weighted, num::Int; kw...)
    newf = x.opt.clamp ? clamped_pdf(logf, x.opt.lo, x.opt.hi) : logf
    mat = metropolis(newf, x.array[:,1], num; kw...)
    n = size(mat,2)
    w1 = convert(eltype(mat), inv(n))
    clamp!(Weighted(mat, fill(w1, n), x.opt))
end

clamped_pdf(logf, lo, hi) = Œ∏ -> all(z -> lo<=z<=hi, Œ∏) ? logf(Œ∏) : convert(eltype(Œ∏),-Inf)


"""
    emcee(logpdf, Œ†0, n) -> Weighted

Takes about `n` samples according to the given `logpdf(::Vector)`.
The initial `Œ†0::Weighted` provides number of walkers, their starting points,
and bounds on `Œ∏` if any.

Sets `res.trace` to be the accepted logpdf values.

Keywords `burn=n/3` number of burn-in steps, `nthin=1` keeps every step.
```
emcee(jlog_exp(1:2), wrandn(2,10), 1_000)
plot(yexp(ans, 1:2))
```
"""
function emcee(logf::Function, vecs::AbsVec, n::Int; burn=div(n,3), verbose=true, kw...)
    isodd(length(vecs)) && push!(vecs, first(vecs))
    res_, acc_, den_, blobs_ = KissMCMC.emcee(logf, vecs; niter=n+burn, nburnin=burn, kw...)
    res, acc, den = KissMCMC.squash_walkers(res_, acc_, den_)
    blobs = if !isnothing(blobs_)
        blobs_2, _ = KissMCMC.squash_walkers(blobs_, acc_)
        reduce(hcat, blobs_2)
    end
    @info string("emcee accepted ", @sprintf("%.2f",100*acc), " percent")
    reduce(hcat, res), den, blobs
end

function emcee(logf::Function, x::Weighted, num::Int; kw...)
    d,k = size(x)
    if k < d+2 # then double the number of walkers & repeat
        xplus = clamp!(hcat(x, x.array .+ 0.01 .* randn.()))
        return emcee(logf, xplus, num; kw...)
    end
    newf = x.opt.clamp ? clamped_pdf(logf, x.opt.lo, x.opt.hi) : logf
    mat, den, _ = emcee(newf, collect.(eachcol(x.array)), num; kw...)
    n = size(mat,2)
    w1 = convert(eltype(mat), inv(n))
    opt = WeightedArrays.wname(_wtrace(x.opt, den), "emcee-"*x.opt.wname)
    clamp!(Weighted(mat, fill(w1, n), opt))
end

_wtrace(o::WeightOpt, v::AbstractVector) =
    WeightedArrays.Setfield.set(o, WeightedArrays.Setfield.@lens(_.trace), v)


#========== My algorithm ==========#
# This aims to find useful candidate points, for further optimisation.

using Distances

using Statistics

"""
    mitchell(y, Œ†0, n) -> Weighted

This finds `n` samples, trying to make `y(Œ†new)[:,a]` far from every `y(Œ†0)[:,b]`,
and returns the combined `hcat(Œ†new, Œ†0)`. Variant of Mitchell's best-candidate algorithm.

Keyword `cand=30` is the number of candidate points at each step, of which the
best `batch=5` get saved. `step = 0.5` is the size of each jump (in `Œ∏` space).
`dist = Distances.SqEuclidean()` is the measure of far away (in `y` space).
"""
function mitchell(yfun, Œ†::Weighted, num::Int; cand=30, batch=5, step=0.5, dist=SqEuclidean())
    d,k = size(Œ†)
    empty!(Œ†.opt.trace)

    # run yfun once, and build a big array to store new points
    y0 = yfun(Œ†).array
    m,k = size(y0)
    ymat = similar(y0, m, k+num)
    ymat[:, 1:k] .= y0

    # similar big matrix for all positions
    Œ∏mat = similar(Œ†.array, d, k+num)
    Œ∏mat[:, 1:k] .= Œ†.array

    @views for col in k+1:batch:k+num
        # randomly select points to perturb, and map new versions to y
        is = rand(1:(col-1), cand)
        Œ∏1 = Œ∏mat[:, is] .+ step .* randn.()
        Œ†1 = clamp!(Weighted(Œ∏1, trues(cand), Œ†.opt))
        y1 = yfun(Œ†1).array

        # pick the candidates furthest from everybody, and save them
        d2 = pairwise(dist, y1, ymat[:, 1:col-1], dims=2) # size cand √ó col-1
        m2 = vec(minimum(d2, dims=2)) # for each candidate, dist to closest point

        b = min(batch, k+num+1-col) # how many to save, usually == batch
        cs = sortperm(m2)[end-b+1:end]

        Œ∏mat[:, col:col+b-1] .= Œ∏1[:, cs]
        ymat[:, col:col+b-1] .= y1[:, cs]
        append!(Œ†.opt.trace, m2[cs])
    end
    s1 = @sprintf("%.3f",mean(Œ†.opt.trace[1:batch]))
    s2 = @sprintf("%.3f",mean(Œ†.opt.trace[end-batch+1:end]))
    @info "mitchell dist = $s1 -> $s2"

    Weighted(Œ∏mat, normalise(vcat(Œ†.weights .* (k√∑2), trues(num))), Œ†.opt)
end

#========== Mark's algorithm ==========#

"""
    transtrum(y, Œ†0, œÉ, n) -> Weighted

This is samples from `p^NML(x)` using `emcee`, and returns the max-likelihood `Œ∏ÃÇ`s,
for a model with gaussian noise `x ~ y(Œ∏) + ùí©(0,œÉ¬≤)`.
These points sample a generalised slab-and-spikes prior, favouring the bondaries.

Initial `Œ†0::Weighted` sets the number of dimensions & the number of walkers.
Keyword `best=true` starts at the best column of `Œ†0`, else a random point (default).

Keywords `iter=30, ftol=0.0` controls LBFGS for finding each `Œ∏ÃÇ` if `method=Optim`.
But default is now `method=LsqFit`, seems a bit better maybe?
Others keywords passed to `emcee`.
```
@_ transtrum(yexp(_, 1:2), soboln(2,5), 0.05, 10_000)
plot(unique(yexp(ans, 1:2), digits=3))
```
"""
function transtrum(yfun, Œ†::Weighted, œÉ::Real, n::Int; kw...)
    _, _, Œ∏s = _transtrum(yfun, Œ†, œÉ, n; kw...)
    n = size(Œ∏s,2)
    w1 = convert(eltype(Œ∏s), inv(n))
    Weighted(Œ∏s, fill(w1, n), Œ†.opt)
end

function _transtrum(yfun, Œ†::Weighted, œÉ::Real, n::Int;
        method=LsqFit, best=false, iter=30, ftol=0.0, eps=1e-2, kw...)
    Œ†.opt.clamp && error("expected an unclamped prior!")
    # Œ∏ = Œ†[:,1]
    x0s = map(yfun, eachcol(Œ†.array))
    # J0 = similar(Œ∏, length(first(x0s)), length(Œ∏)) # not thread-safe!
    Jcaches = [similar(Œ†.array, length(first(x0s)), size(Œ†,1)) for _ in 1:Threads.nthreads()]

    xs, dens, Œ∏s = emcee(x0s, n; hasblob=true, kw...) do x
        Œ∏0 = best ? Œ†.array[:, find_closest_index(x, x0s)] : randn!(Œ†[:,1])
        if method == LsqFit
            lsqfit_gauss_mle(x, yfun, Œ∏0, œÉ, Jcaches[Threads.threadid()])
        elseif method == Optim
            optim_gauss_mle(x, yfun, Œ∏0, œÉ; iter=iter, ftol=ftol) # gives (logpdf, Œ∏)
        elseif method == ForwardDiff
            grad_gauss_mle(x, yfun, Œ∏0, œÉ; iter=iter, eps=eps)
        end
    end
end

using LsqFit: LsqFit # Mark's reccommendation, Levenberg-Marquardt alg:

function lsqfit_gauss_mle(z::AbsVec, yfun::Function, Œ∏0::AbsVec, œÉ::Real,
        J=similar(Œ∏0, length(yfun(Œ∏0)), length(Œ∏0)))
    lsq_fun(_,Œ∏) = yfun(Œ∏)
    lsq_jac(_,Œ∏) = ForwardDiff.jacobian!(J, yfun, Œ∏)
    fit = LsqFit.curve_fit(lsq_fun, lsq_jac, 1:0, z, Œ∏0)
    sum(abs2, fit.resid)/(-2 * œÉ^2), fit.param
end

using Optim # my go-to LBFGS idea, for finding closest Œ∏:

function optim_gauss_mle(z::AbsVec, yfun::Function, Œ∏0::AbsVec, œÉ::Real; iter=30, ftol=0.0)
    od = OnceDifferentiable(Œ∏0, autodiff=:forward) do Œ∏
        sum(abs2.(yfun(Œ∏) .- z))
    end
    opt = LBFGS(linesearch = Optim.BackTracking()) # default is linesearch = Optim.HagerZhang()
    res = Optim.optimize(od, Œ∏0, opt, Optim.Options(iterations=iter, f_abstol=ftol))
    Optim.minimum(res)/(-2 * œÉ^2), Optim.minimizer(res)
end

using ForwardDiff # my super-crude gradient descent, a sanity check:

function grad_gauss_mle(z::AbsVec, yfun::Function, Œ∏0::AbsVec, œÉ::Real; iter=10^4, eps=1e-2)
    for _ in 1:iter
        grad = ForwardDiff.gradient(Œ∏0) do Œ∏
            sum(abs2.(yfun(Œ∏) .- z))
        end
        Œ∏0 .-= eps .* grad
    end
    sum(abs2.(yfun(Œ∏0) .- z))/(-2 * œÉ^2), Œ∏0
end

function find_closest_index(x::AbsVec, x0s::AbsVec{<:AbsVec})
    @assert length(x) == length(first(x0s))
    ind, dist = 0, Inf
    @inbounds for j in 1:length(x0s)
        dj = 0.0
        x0j = x0s[j]
        for Œº in 1:length(x)
            dj += (x[Œº] - x0j[Œº])^2
        end
        ind = ifelse(dj<dist, j, ind)
        dist = min(dj, dist)
    end
    ind
end

#=

best=true seems to lead this (from notes/sumexp03.jl) to run into singularities:
@time m3 = transtrum(yexp(t26), 4 .* soboln(3,50), 0.1, 200_000; nthin=20)

ERROR: TaskFailedException:
SingularException(3)
Stacktrace:
 [1] checknonsingular at /Applications/Julia-1.5.app/Contents/Resources/julia/share/julia/stdlib/v1.5/LinearAlgebra/src/factorization.jl:19 [inlined]
...
 [6] \(::Array{Float64,2}, ::Array{Float64,1}) at /Applications/Julia-1.5.app/Contents/Resources/julia/share/julia/stdlib/v1.5/LinearAlgebra/src/generic.jl:1116
 [7] levenberg_marquardt(::NLSolversBase.OnceDifferentiable{Array{Float64,1},Array{Float64,2},Array{Float64,1}}, ::Array{Float64,1}; x_tol::Float64, g_tol::Float64, maxIter::Int64, lambda::Float64, tau::Float64, lambda_increase::Float64, lambda_decrease::Float64, min_step_quality::Float64, good_step_quality::Float64, show_trace::Bool, lower::Array{Float64,1}, upper::Array{Float64,1}, avv!::Nothing) at /Users/me/.julia/packages/LsqFit/LeVh7/src/levenberg_marquardt.jl:126

 =#

"""
    transtrum(f, Œ†0) -> Weighted
    transtrum(f, Œ†0, n; repeat)

Version for discrete outcomes: `f(Œ†0)` should be the likelihood matrix.
This just scans all `x ‚àà X` and stores the closest `Œ∏` point to each,
weighted by `p^NML(x)`.

Nothing monte-carlo about it, and no need to specify a number of samples!
But you can sub-sample to keep just `n`, especially for large `repeat`.

Keyword `iter=100` controls LBFGS for finding each `Œ∏ÃÇ`, this is
not as robust as I would like!
```
@_ transtrum(coin(_,10), sobol(1,5))        # 11 points for 11 outcomes
@_ transtrum(coin(_,1), sobol(1,5), repeat=10)
@_ nlopt!(mutual(coin(_,10)), sobol(1,20))  # 5 points, maximal I(X;Œò)
```
Keyword `repeat=5` uses a more efficient method than `repeat(f(_),5)`.
"""
function transtrum(f, Œ†::Weighted, n::Union{Nothing,Int}=nothing; iter=100, repeat=nothing)
    Œ†.opt.clamp && @warn "not sure this works well with clamped priors"

    isnothing(repeat) || return transtrum_repeat(f, Œ†, n, repeat; iter=iter)
    n isa Int && return transtrum_repeat(f, Œ†, n, 1; iter=iter)

    d,k = size(Œ†)
    like = f(Œ†).array
    m,_ = size(like)

    # outputs
    phi = similar(Œ†.array, d, m)
    pNML = similar(Œ†.weights, m)

    Threads.@threads for i in 1:m
        # start at the closest point, why not?
        ai = argmax(like[i,:])
        Œ∏0 = Œ†.array[:,ai]

        # solve
        od = OnceDifferentiable(Œ∏0, autodiff=:forward) do Œ∏
            pie = WeightedMatrix(Œ∏, [1.0], Œ†.opt) |> clamp!
            -log(f(pie).array[i,1]) # maximum likelihood, minimum -log(f())
            # -f(pie).array[i,1]
        end
        opt = LBFGS(linesearch = Optim.BackTracking())
        res = Optim.optimize(od, Œ∏0, opt, Optim.Options(iterations=iter))

        # save
        phi[:,i] .= Optim.minimizer(res)
        pNML[i] = exp(-Optim.minimum(res)) # optimising in terms of log-prob a little better
        # pNML[i] = -Optim.minimum(res)
    end

    # sanitise
    ok = map(isfinite, pNML)
    Weighted(phi[:, ok], normalise!(pNML[ok]), Œ†.opt) |> clamp! |> unique
end

function transtrum_repeat(f, Œ†::Weighted, n, repeat::Int; iter=100)
    d,k = size(Œ†)
    like1 = f(Œ†)::Weighted
    m1,_ = size(like1) # m1 is number of outcomes of 1 repetition

    zz = part_pos(repeat, m1) # don't ask why it's called zz everywhere
    mul = multi_pos(repeat, m1)
    like2 = Base.repeat(like1, repeat).array
    if n isa Int
        n < length(zz) || @warn "you asked for $n samples, out of $(length(zz)) total ($m1 outcomes with $repeat repetitions)"
        list = rand(1:length(zz), n)
        zz = zz[list]
        mul = mul[list]
        like2 = like2[list, :]
    end
    m2 = length(zz) # m2 is the total, after all repetitions

    # outputs
    phi = similar(Œ†.array, d, m2)
    pNML = similar(Œ†.weights, m2)

    _transtrum_repeat(f,Œ†,repeat,iter,d,k,like2,zz,mul,m2,phi,pNML)
end # because zz is not type-stable
function _transtrum_repeat(f,Œ†,repeat,iter,d,k,like2,zz,mul,m2,phi,pNML)

    Threads.@threads for i in 1:m2
    # for i in 1:m2
        # start at the closest point, why not?
        ai = argmax(like2[i,:])
        Œ∏0 = Œ†.array[:,ai]

        # solve
        od = OnceDifferentiable(Œ∏0, autodiff=:forward) do Œ∏
            pie = WeightedMatrix(Œ∏, [1.0], Œ†.opt) |> clamp!

            # now, instead of calculating the whole repeated likelihood, only i-th row:
            loglikepie = f(pie).array
            -log(mul[i]) - sum(log, loglikepie[x,1] for x in zz[i])
        end
        opt = LBFGS(linesearch = Optim.BackTracking())
        res = Optim.optimize(od, Œ∏0, opt, Optim.Options(iterations=iter))

        # save
        phi[:,i] .= Optim.minimizer(res)
        pNML[i] = exp(-Optim.minimum(res))
    end

    # sanitise
    ok = map(isfinite, pNML)
    Weighted(phi[:, ok], normalise!(pNML[ok]), Œ†.opt) |> clamp! |> unique
end

#= # This is copied from here:

@noinline function _repeat_pos(mat,m,zz,out,k,mul)
    for (r,z) in enumerate(zz) # row of the final likelihood
        out[r,:] .= mul[r]
        for x in z             # is a product of like of individual outcomes
            out[r,:] .*= mat[x,:]
        end
    end
    out
end

=#

"""
    transtrum_exp(Œ†0, times, œÉ, n)

This is like `transtrum(y, Œ†0, œÉ, n)` but uses the analytic `jacobi_exp`
instead of ForwardDiff, for LsqFit. Example:
```
ty1 = transtrum_exp(soboln(2,10), 1:2, 0.1, 10^4; nthin=10)
ty2 = @_ transtrum(yexp(_, 1:2), soboln(2,10), 0.1, 10^4; nthin=10)
plot(yexp(ty1, 1:2))
plot!(yexp(ty2, 1:2) .+ [0, 0.25])
```
"""
transtrum_exp(Œ†::Weighted, times::AbsVec, œÉ::Real, n::Int; kw...) = 
    first(transtrum_exp_pair(Œ†, times, œÉ, n; kw...))

function transtrum_exp_pair(Œ†::Weighted, times::AbsVec, œÉ::Real, n::Int; best::Bool=false, kw...)
    Œ†.opt.clamp && error("expected an unclamped prior!")
    x0s = collect.(eachcol(yexp(Œ†, times).array))

    Jcaches = [similar(Œ†.array, length(first(x0s)), size(Œ†,1)) for _ in 1:Threads.nthreads()]
    Œ∏caches = [similar(Œ†.array, size(Œ†,1)) for _ in 1:Threads.nthreads()]
    ycaches = [similar(Œ†.array, length(times)) for _ in 1:Threads.nthreads()]

    xs, dens, Œ∏s = emcee(x0s, n; hasblob=true, kw...) do x # NB emcee is multi-threaded!
        Œ∏0 = best ? Œ†.array[:, find_closest_index(x, x0s)] : randn!(Œ†[:,1])
        tid = Threads.threadid()
        lsqfit_exp_mle(x, Œ∏0, times, œÉ, Jcaches[tid], Œ∏caches[tid], ycaches[tid])
    end

    n = size(Œ∏s,2)
    w1 = convert(eltype(Œ∏s), inv(n))
    Weighted(Œ∏s, fill(w1, n), Œ†.opt), Weighted(xs) # return xs too, for p_NML scatter plots
end

function lsqfit_exp_mle(z::AbsVec, Œ∏0::AbsVec, times::AbsVec, œÉ::Real,
        Jcache::AbsMat, Œ∏cache::AbsVec, ycache::AbsVec)
    lsq_fun(_,Œ∏) = yexp!(ycache, Œ∏, times)
    lsq_jac(_,Œ∏) = jacobi_exp!(Jcache, Œ∏, times; cache=Œ∏cache) # these two share a lot of work!
    fit = LsqFit.curve_fit(lsq_fun, lsq_jac, 1:0, z, Œ∏0)
    sum(abs2, fit.resid)/(-2 * œÉ^2), fit.param
end
